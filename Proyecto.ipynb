{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "571b565e",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"figuras/escudo_uach.png\" alt=\"UACh\" width=\"70\"/>\n",
    "    </p>\n",
    "\n",
    "# Proyecto de ACUS 220 Acústica Computacional con Python  \n",
    "\n",
    "### Primer Hito de Entrega  \n",
    "\n",
    "**Integrantes:** José Manuel Godoy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833e861f",
   "metadata": {},
   "source": [
    "## Objetivos del Proyecto  \n",
    "\n",
    "**Objetivo General:**  \n",
    "- Entrenar y evaluar un modelo de clasificación de escenas acústicas capaz de distinguir entre escenas de vehículos, interiores y exteriores usando el conjunto TUT Acoustic Scenes 2016 - Task 1, y documentar el proceso reproducible en Python.\n",
    "\n",
    "**Objetivos Específicos:**  \n",
    "- OE1: Realizar análisis exploratorio y documentarlo.\n",
    "- OE2: Preprocesamiento y extracción de características.\n",
    "- OE3: Desarrollar y entrenar modelos CNN/CRNN sobre espectrogramas\n",
    "- OE4: Evaluar con métricas relevantes (accuracy, macro-F1, matriz de confusión, recall por clase) y análisis de errores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfa5889",
   "metadata": {},
   "source": [
    "## Estado del Arte  \n",
    "\n",
    "- TUT Acoustic Scenes 2016 fue introducido en DCASE 2016. Contiene escenas de audio en distintas escenas cotidianas y ha sido ampliamente utilizado en tareas de clasificación de escenas acústicas. Este proyecto inspiró versiones en años posteriores con más datos y mayor complejidad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676f9f9a",
   "metadata": {},
   "source": [
    "## Materiales y Métodos  \n",
    "\n",
    "### Materiales  \n",
    "- TUT sound events 2016 Task 1, consiste en segmentos de audio correspondientes a 15 clases de escenas acústicas, las cuales son:\n",
    "    - **Vehículos:** Bus, auto, Tren y Tranvía\n",
    "    - **Exteriores:** Centro de la ciudad, bosque, playa, área residencial y parque urbano.\n",
    "    - **Interiores:** Cafe/Restaurant, tienda, casa, biblioteca, estación de metro y oficina\n",
    "- **Herramientas a utilizar:** Python, librosa, soundfile, numpy, pandas, matplotlib, scikit-learn, PyTorch\n",
    "\n",
    "### Metodología  \n",
    "Describir paso a paso cómo se abordará el problema.\n",
    "1. **Carga y verificación de audio**: lectura de los archivos del conjunto TUT Sound Events 2016, asegurando formato consistente (tasa de muestreo, canal único, duración).\n",
    "2. **Análisis exploratiorio de los datos:** obtención de estadísticas básicas (número de muestras por clase, duración promedio) y visualización de ejemplos mediante espectrogramas.\n",
    "3. **Preprocesamiento:** normalización de amplitud y recorte o relleno de las señales a una duración uniforme para facilitar el procesamiento.\n",
    "4. **Extracción de caracterísitcas (features):** generación de representaciones log-mel spectrogram o uso de embeddings preentrenados que describan la información espectral y temporal relevante del audio.\n",
    "5. **Split de datos:** separación de los audios en subconjuntos de entrenamiento, validación y prueba, manteniendo una distribución equilibrada entre clases.\n",
    "6. **Entrenar modelo:** desarrollo y entrenamiento de una red neuronal convolucional (CNN) sobre los mel-spectrogramas para la clasificación de escenas acústicas.\n",
    "7. **Evaluación:** medición del rendimiento mediante métricas como accuracy, macro-F1, recall y precision por clase, además de la matriz de confusión para analizar los errores de clasificación.\n",
    "\n",
    "**Ejemplo plan de trabajo:**  \n",
    "\n",
    "| Actividad                        | Fecha estimada |\n",
    "|----------------------------------|----------------|\n",
    "| Revisión bibliográfica           | 10/10/2025     |\n",
    "| Recolección de datos en terreno  | 20/10/2025     |\n",
    "| Procesamiento de datos en Python | 30/10/2025     |\n",
    "| Entrenamiento del modelo         | 15/11/2025     |\n",
    "| Redacción de informe parcial     | 25/11/2025     |  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee86f3c",
   "metadata": {},
   "source": [
    "## Referencias Bibliográficas  \n",
    "\n",
    "- Mesaros, A., Heittola, T., & Virtanen, T. (2016). TUT Acoustic scenes 2016, Development dataset [Data set]. Zenodo. https://doi.org/10.5281/zenodo.45739\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acus_python",
   "language": "python",
   "name": "acus_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
